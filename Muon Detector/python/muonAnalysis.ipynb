{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from math import *\n",
    "from scipy.optimize import curve_fit, minimize\n",
    "from scipy.stats import poisson\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "from matplotlib.ticker import FormatStrFormatter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data\n",
    "This section comprises the loading of data runs. The data is saved in sections of 1.2 hours and so a second function was created to load multiple files together."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "channels = [73, 152, 234, 316, 395, 593, 807]\n",
    "lifetimes_ns = [100, 200, 300, 400, 500, 750, 1000]\n",
    "\n",
    "lifetimes_us = [el / 1000 for el in lifetimes_ns]\n",
    "\n",
    "def Linear(x, m, c):\n",
    "    return m*x + c\n",
    "\n",
    "calib_pars, calib_cov = curve_fit(Linear, channels, lifetimes_us)\n",
    "\n",
    "print(calib_pars)\n",
    "print(np.sqrt(calib_cov[0][0]), np.sqrt(calib_cov[1][1]))\n",
    "\n",
    "# Error on channel corresponds to peak curve on MCA, error on lifetime corresponds to the uncertainty on the pulse generator / oscilloscope.\n",
    "plt.errorbar(channels, lifetimes_us, xerr=4, yerr=10/1000, fmt=\".\", lw=1, capsize=3, color=\"indianred\", label=\"Calibration data\")\n",
    "\n",
    "plt.plot(np.arange(0, 900), Linear(np.arange(0, 900), calib_pars[0], calib_pars[1]), color=\"cornflowerblue\", label=\"Least Squares fit\")\n",
    "\n",
    "plt.xlabel(\"Channel\")\n",
    "plt.ylabel(\"Lifetime ($\\mu$s)\")\n",
    "plt.margins(0)\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ApplyCalibration(channelNumbers):\n",
    "    lifetimes = []\n",
    "    for channel in channelNumbers:\n",
    "        lifetimes.append(calib_pars[0] * channel + calib_pars[1])\n",
    "\n",
    "    return lifetimes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def LoadExperimentRun(path, numberOfChannelsToRemove, calibrate=False):\n",
    "\n",
    "    channelCounts = np.loadtxt(path)[numberOfChannelsToRemove[0]:-numberOfChannelsToRemove[1]]\n",
    "\n",
    "    channelNumbers = np.arange(numberOfChannelsToRemove[0], len(channelCounts)+numberOfChannelsToRemove[0])\n",
    "\n",
    "    if calibrate is True:\n",
    "        channelNumbers = ApplyCalibration(channelNumbers)\n",
    "\n",
    "    return (channelNumbers, channelCounts)\n",
    "\n",
    "def LoadMultipleRuns(basePath, filenamePrefix, numberOfChannelsToRemove, calibrate=False):\n",
    "    #print(basePath + filenamePrefix + \"*.txt\")\n",
    "    filePaths = glob(basePath + filenamePrefix + \"*.txt\")\n",
    "    \n",
    "    filePaths.sort()\n",
    "    print(f\"Loading:\\n{filePaths}\")\n",
    "\n",
    "    channelsSample = LoadExperimentRun(filePaths[0], numberOfChannelsToRemove, calibrate)[0]\n",
    "    countsSample = LoadExperimentRun(filePaths[0], numberOfChannelsToRemove)[1]\n",
    "\n",
    "    channelCounts = np.zeros(np.shape(countsSample))\n",
    "    for filePath in filePaths:\n",
    "        channelCounts = channelCounts + LoadExperimentRun(filePath, numberOfChannelsToRemove, calibrate)[1]\n",
    "\n",
    "    occurances = 0\n",
    "    for el in channelCounts:\n",
    "        occurances += el\n",
    "\n",
    "\n",
    "    print(f\"{occurances} occurances\")\n",
    "        \n",
    "    return (channelsSample, channelCounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example loading\n",
    "data = LoadMultipleRuns(\"../data/\", \"autoDetection_T-800mV_40ns_DMH_\", (100, 1), calibrate=False)\n",
    "\n",
    "plt.scatter(data[0], data[1], marker=\".\", color=\"cornflowerblue\")\n",
    "plt.xlabel(\"Channel\")\n",
    "plt.ylabel(\"Counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Analysis Pipeline\n",
    "## Re-Binning Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RebinData(data, binFraction, method=\"mean\", verbose=False):\n",
    "    # Inputs are the data output from LoadExperimentRun, and a binFraction which is a number between 0 and 1 which determines how large the bins are\n",
    "\n",
    "    bins = list(data[0])\n",
    "    data = list(data[1])\n",
    "\n",
    "    binWidth = (bins[-1] - bins[0]) * binFraction\n",
    "\n",
    "    lowerBound = bins[0]\n",
    "    numberOfBins = floor((bins[-1] - bins[0]) / binWidth)\n",
    "\n",
    "    newBins = np.arange(lowerBound, binWidth * (numberOfBins + 1), binWidth)\n",
    "    newBins = [el + binWidth / 2 for el in newBins]\n",
    "    #newBins = [floor(el) for el in newBins]\n",
    "\n",
    "    #topBound = floor(newBins[-1] + binWidth)\n",
    "    #newBins.append(topBound)\n",
    "    \n",
    "    binnedData = []\n",
    "\n",
    "    for i, binBound in enumerate(newBins):\n",
    "        #print(f\"{i}/{len(newBins)}\")\n",
    "        if i < len(newBins)-1:\n",
    "            #binnedData.append(list(data[newBins[i]:newBins[i+1]]))\n",
    "\n",
    "            binnedData.append(np.array(data)[np.where((bins > newBins[i]) & (bins < newBins[i+1]))])\n",
    "\n",
    "    # binnedData is a an array which each row containing a bin\n",
    "    binnedData = np.array(binnedData, dtype=\"object\")\n",
    "\n",
    "    averagedBinnedData = []\n",
    "    binnedDataLowerUncertainty = []\n",
    "    binnedDataUpperUncertainty = []\n",
    "    for row in tqdm(binnedData, disable= not verbose):\n",
    "\n",
    "        avg = AverageDataInBin(row, method, verbose=verbose)\n",
    "        averagedBinnedData.append(avg[0])\n",
    "        binnedDataLowerUncertainty.append(avg[1])\n",
    "        binnedDataUpperUncertainty.append(avg[2])\n",
    "\n",
    "    return (newBins[:-1], averagedBinnedData, binnedDataLowerUncertainty, binnedDataUpperUncertainty)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods for Averaging Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AverageDataInBin(dataInBin, method=\"mean\", verbose=False):\n",
    "\n",
    "    if method == \"mean\":\n",
    "        averagedRow = sum(dataInBin) / len(dataInBin)\n",
    "        uncertainty = np.std(dataInBin)\n",
    "\n",
    "        upperUncertainty = lowerUncertainty = uncertainty / np.sqrt(len(dataInBin))\n",
    "        \n",
    "\n",
    "    elif method == \"gaussian\":\n",
    "        averagedRow = np.mean(dataInBin)\n",
    "        variance = np.sqrt(averagedRow)\n",
    "        standardError = variance / np.sqrt(len(dataInBin))\n",
    "\n",
    "        upperUncertainty = lowerUncertainty = standardError\n",
    "\n",
    "    elif method == \"MLE\":\n",
    "        \n",
    "        # Finds the maximum likelihood for mu in a poisson distribution\n",
    "        # To do this we can find the minimum of the negative log likelihood\n",
    "        model = minimize(MLE_negativeLogLikelihood, np.max(dataInBin) / 2, args=dataInBin, method=\"Nelder-Mead\")\n",
    "        \n",
    "        averagedRow = model.x\n",
    "        lowerUncertainty, upperUncertainty = poisson.interval(0.68, averagedRow) # 1 sigma uncertainty\n",
    "        lowerUncertainty, upperUncertainty = (lowerUncertainty / np.sqrt(len(dataInBin)), upperUncertainty / np.sqrt(len(dataInBin)))\n",
    "        \n",
    "        if verbose:\n",
    "            print(averagedRow, uncertainty)\n",
    "\n",
    "    return averagedRow, lowerUncertainty, upperUncertainty\n",
    "\n",
    "\n",
    "# Method of Maximum Likelihood Estimation - Calculate the likelihood of the pmf function of a given mu for each datapoint in the bin\n",
    "def MLE_negativeLogLikelihood(lamda, data):\n",
    "\n",
    "    logLikelihoods = []\n",
    "    for el in data:\n",
    "        logLikelihoods.append(poisson.pmf(k=el, mu = lamda))\n",
    "\n",
    "    return -1 * np.sum(logLikelihoods)\n",
    "\n",
    "\n",
    "def FindFitParameters(data, FittingFunction, sigma, initialPars=None):\n",
    "\n",
    "    if sigma == None:\n",
    "        pars, cov = curve_fit(FittingFunction, data[0], data[1], initialPars)\n",
    "\n",
    "    else:\n",
    "        pars, cov = curve_fit(FittingFunction, data[0], data[1], initialPars, sigma=sigma, absolute_sigma=True)\n",
    "\n",
    "    return pars, cov\n",
    "    \n",
    "def ExponentialCurve(x, a, b, c, d):\n",
    "    y = []\n",
    "    \n",
    "    for point in x:\n",
    "        y.append(a * exp(- b * point + c) + d)\n",
    "    \n",
    "    return y\n",
    "\n",
    "#, [10, -0.01, 0.1, 15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example rebinning\n",
    "\n",
    "data = LoadMultipleRuns(\"../data/\", \"autoDetection_T-800mV_40ns_DMH_\", (100, 1), calibrate=False)\n",
    "\n",
    "rebinnedData = RebinData(data, binFraction=0.05, method=\"gaussian\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(6, 12))\n",
    "ax1, ax2 = axes\n",
    "\n",
    "ax1.scatter(data[0], data[1], marker=\".\", color=\"cornflowerblue\")\n",
    "ax1.set_title(\"Raw Data\")\n",
    "\n",
    "ax2.errorbar(rebinnedData[0], rebinnedData[1], yerr=rebinnedData[2], fmt=\".\", color=\"cornflowerblue\")\n",
    "ax2.set_title(\"Binned Data, size: 5%\")\n",
    "\n",
    "for ax in axes:\n",
    "    ax.set_ylim(0, 40)\n",
    "    ax.set_xlabel(\"Channels\")\n",
    "    ax.set_ylabel(\"Counts\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding optimum bin sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fakeDataX = np.linspace(0, 1000, 1000)\n",
    "fakeDataY = ExponentialCurve(fakeDataX, 1, 0.01, 0, 0)\n",
    "\n",
    "\n",
    "rebinnedFakeData_20 = RebinData((fakeDataX, fakeDataY), binFraction=0.20, method=\"gaussian\")\n",
    "rebinnedFakeData_15 = RebinData((fakeDataX, fakeDataY), binFraction=0.15, method=\"gaussian\")\n",
    "rebinnedFakeData_10 = RebinData((fakeDataX, fakeDataY), binFraction=0.10, method=\"gaussian\")\n",
    "rebinnedFakeData_5 = RebinData((fakeDataX, fakeDataY), binFraction=0.05, method=\"gaussian\")\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(8,8), sharex=True, sharey=True)\n",
    "axes = axes.reshape(4)\n",
    "\n",
    "titles = [\"20% bin size\", \"15% bin size\", \"10% bin size\", \"5% bin size\"]\n",
    "fakeData = [rebinnedFakeData_20, rebinnedFakeData_15, rebinnedFakeData_10, rebinnedFakeData_5]\n",
    "\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.plot(fakeDataX, fakeDataY, zorder=0, color=\"cornflowerblue\")\n",
    "\n",
    "    ax.errorbar(fakeData[i][0], fakeData[i][1], yerr=fakeData[i][2], color=\"indianred\", fmt=\".\", capsize=3, linewidth=1)\n",
    "\n",
    "    ax.set_title(titles[i])\n",
    "\n",
    "axes[2].set_xlabel(\"x (arb.)\")\n",
    "axes[3].set_xlabel(\"x (arb.)\")\n",
    "axes[0].set_ylabel(\"y (arb.)\")\n",
    "axes[2].set_ylabel(\"y (arb.)\")\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find Best Bin Size\n",
    "This section handles the plotting of the uncertainties on the parameters of a least squares fit for **any input function**. This plot can be used to estimate the most optimal bin-size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindOptimumBinFraction(data, binFractions, FitFunction=ExponentialCurve, method=\"gaussian\", initialPars=None):\n",
    "\n",
    "    parameterUncertainties = []\n",
    "\n",
    "    print(\"Assessing bin fractions\")\n",
    "    for binFrac in binFractions:\n",
    "        #print(f\"Testing binFrac: {binFrac}\", end=\"\\r\")\n",
    "        rebinnedData = RebinData(data, binFrac, method=method)\n",
    "\n",
    "        x = rebinnedData[0]\n",
    "        y = rebinnedData[1]\n",
    "        y = np.squeeze(y)\n",
    "\n",
    "        tol = 0.01\n",
    "        newSigma = []\n",
    "        for el in rebinnedData[2]:\n",
    "            if el < tol:\n",
    "                newSigma.append(tol)\n",
    "            else:\n",
    "                newSigma.append(el)\n",
    "\n",
    "        pars, cov = FindFitParameters((x,y), FitFunction, sigma=newSigma, initialPars=initialPars)\n",
    "\n",
    "        for i in range(len(cov)):\n",
    "            parameterUncertainties.append(np.sqrt(cov[i][i]))\n",
    "\n",
    "    # Normalise and make array of uncertainties\n",
    "\n",
    "    parameterUncertainties = np.array(parameterUncertainties).reshape(len(binFractions), len(cov))\n",
    "\n",
    "    #for i, uncertainties in enumerate(parameterUncertainties.T):\n",
    "        \n",
    "        #parameterUncertainties[:,i] = [el / max(uncertainties) for el in uncertainties]\n",
    "    \n",
    "    return (binFractions, parameterUncertainties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = (fakeDataX, fakeDataY)\n",
    "\n",
    "binFractions = np.linspace(0.06, 0.01, 1000)\n",
    "\n",
    "_, parameterUncertainties = FindOptimumBinFraction(data, binFractions, initialPars=[1, 0.01, 0, 0])\n",
    "\n",
    "#plt.plot(binFractions, parameterUncertainties.T[0])\n",
    "\n",
    "fig, axes = plt.subplots(4, 1, figsize=(6,8), sharex=True)\n",
    "\n",
    "fig.text(0.01, 0.5, 'Fit Parameter Normalised Uncertainty', va='center', rotation='vertical')\n",
    "\n",
    "parameterNames = [\"a\", \"b\", \"c\", \"d\"]\n",
    "yLimits = [0.25, 0.25, 0.25, 0.25]\n",
    "\n",
    "for ax, uncertainties, parameterName, yLim in zip(axes, parameterUncertainties.T, parameterNames, yLimits):\n",
    "\n",
    "    uncertainties = np.array(uncertainties)\n",
    "\n",
    "    uncertainties[uncertainties == inf] = np.nan\n",
    "\n",
    "    uncertainties = uncertainties / np.nanmax(uncertainties)\n",
    "\n",
    "    ax.plot(binFractions, uncertainties, color=\"indianred\")\n",
    "\n",
    "    ax.margins(0)\n",
    "    ax.set_ylabel(parameterName, rotation=0)\n",
    "\n",
    "\n",
    "    #ax.set_ylim(0, yLim)\n",
    "    ax.axvline(x=0.045,ymin=0,ymax=1,c=\"cornflowerblue\", linestyle=\"dashed\", clip_on=False, label=\"Chosen Bin Fraction\")\n",
    "\n",
    "    if ax == axes[-1]:\n",
    "        ax.set_xlabel(\"Bin Size (fraction of total number of ch.)\")\n",
    "\n",
    "    if ax == axes[0]:\n",
    "        ax.set_title(\"Fit parameter uncertainties for: $f(x) = a e^{(b x + c)} + d$\")\n",
    "        ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gaussian Approximation of the Poisson Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = LoadMultipleRuns(\"../data/\", \"autoDetection_T-800mV_40ns_DMH_\", (100, 200), calibrate=True)\n",
    "\n",
    "binFraction = 0.045\n",
    "binSize = binFraction * data[0][-1]\n",
    "print(f\"Bin Size: {binSize:0.2f} us\")\n",
    "\n",
    "rebinnedData = RebinData(data, binFraction, method=\"gaussian\", verbose=False)\n",
    "\n",
    "x = rebinnedData[0]\n",
    "y = rebinnedData[1]\n",
    "y = np.squeeze(y)\n",
    "\n",
    "yErr = np.array(list(zip(rebinnedData[2], rebinnedData[3]))).T\n",
    "yErr = np.squeeze(yErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gPars, gCov = FindFitParameters((x,y), ExponentialCurve, sigma=rebinnedData[2], initialPars=[15, 0.05, 0.1, 3])\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,8))\n",
    "\n",
    "print(gPars)\n",
    "\n",
    "fitCurveX = np.linspace(0, max(x), 1000)\n",
    "ax.plot(fitCurveX, ExponentialCurve(fitCurveX, gPars[0], gPars[1], gPars[2], gPars[3]), color=\"cornflowerblue\", label=\"Least Squares Fit\")\n",
    "\n",
    "#ax.plot(fitCurveX, ExponentialCurve(fitCurveX, 15, -0.05, 0.1, 3), color=\"orange\", label=\"Test Function\")\n",
    "\n",
    "ax.errorbar(x, y, yerr=yErr, fmt=\".\", capsize=3, linewidth=1, color=\"indianred\", label=\"Gaussian Averaged Data\")\n",
    "\n",
    "ax.set_xlabel(\"Decay Time ($\\mu s$)\")\n",
    "ax.set_ylabel(\"Counts\")\n",
    "ax.legend()\n",
    "ax.set_title(f\"Gaussian Approximation\\nBin Size: {binSize:0.2f} microseconds\")\n",
    "ax.margins(x=0)\n",
    "ax.hlines([0], 0, max(x) + 0.01*max(x), color=\"grey\", ls=\"dashed\")\n",
    "#ax.set_yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Half-life estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decayConst = gPars[1]\n",
    "\n",
    "print(f\"Half-life: {log(2) / decayConst:0.5f} us\")\n",
    "\n",
    "decayConstantUncertainty = np.sqrt(gCov[1][1])\n",
    "\n",
    "print(f\"Uncertainty (propagated from fit): {np.sqrt(decayConstantUncertainty**2 * (log(2) / decayConst**2)**2)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"Lifetime: {1/ decayConst:0.5f} us\")\n",
    "print(f\"Uncertainty (propagated from fit): {np.sqrt(decayConstantUncertainty**2 * (1 / decayConst**2)**2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = LoadMultipleRuns(\"../data/\", \"autoDetection_T-800mV_40ns_DMH_\", (90, 200), calibrate=True)\n",
    "\n",
    "binFraction = 0.045\n",
    "binSize = binFraction * data[0][-1]\n",
    "print(f\"Bin Size: {binSize:0.2f} us\")\n",
    "\n",
    "rebinnedData = RebinData(data, binFraction, method=\"MLE\", verbose=False)\n",
    "\n",
    "x = rebinnedData[0]\n",
    "y = rebinnedData[1]\n",
    "y = np.squeeze(y)\n",
    "\n",
    "yErr = np.array(list(zip(rebinnedData[2], rebinnedData[3]))).T\n",
    "yErr = np.squeeze(yErr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = []\n",
    "for a, b in zip(np.squeeze(rebinnedData[2]), np.squeeze(rebinnedData[3])):\n",
    "    sig.append(np.sqrt(a**2 + b**2))\n",
    "\n",
    "\n",
    "pars, cov = FindFitParameters((x, y), ExponentialCurve, sigma=sig, initialPars=[19, 0.05, 0.1, 1])\n",
    "\n",
    "print(pars)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(6,8))\n",
    "\n",
    "fitCurveX = np.linspace(0, max(x), 1000)\n",
    "\n",
    "#ax.plot(fitCurveX, ExponentialCurve(fitCurveX, 18, -0.05, 0.1, 1), color=\"orange\", label=\"Test Function\")\n",
    "\n",
    "ax.plot(fitCurveX, ExponentialCurve(fitCurveX, pars[0], pars[1], pars[2], pars[3]), color=\"cornflowerblue\", label=\"Least Squares Fit\")\n",
    "\n",
    "ax.errorbar(x, y, yerr=yErr, fmt=\".\", capsize=3, linewidth=1, color=\"indianred\", label=\"MLE averaged data\")\n",
    "\n",
    "ax.set_xlabel(\"Decay Time ($\\mu s$)\")\n",
    "ax.set_ylabel(\"Counts\")\n",
    "ax.legend()\n",
    "ax.set_title(f\"Maximum Likelihood Estimation Method (Poisson)\\nBin Size: {binSize:0.2f} microseconds\")\n",
    "ax.margins(x=0)\n",
    "ax.hlines([0], 0, max(x) + 0.01*max(x), color=\"grey\", ls=\"dashed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Half-life estimation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decayConst = pars[1]\n",
    "\n",
    "print(f\"Half-life: {log(2) / decayConst:0.5f} us\")\n",
    "\n",
    "decayConstantUncertainty = np.sqrt(cov[1][1])\n",
    "\n",
    "print(f\"Uncertainty (propagated from fit): {np.sqrt(decayConstantUncertainty**2 * (log(2) / decayConst**2)**2)}\")\n",
    "\n",
    "print()\n",
    "\n",
    "print(f\"Lifetime: {1/ decayConst:0.5f} us\")\n",
    "print(f\"Uncertainty (propagated from fit): {np.sqrt(decayConstantUncertainty**2 * (1 / decayConst**2)**2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Muon flux as a function of Zenith Angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zenithData = np.loadtxt(\"../data/zenithAngles.txt\", skiprows=1)\n",
    "\n",
    "angles = zenithData[:,0] # degrees\n",
    "anglesUncertainty = 2 # degrees\n",
    "\n",
    "counts = zenithData[:,1]\n",
    "countsUncertainties = [np.sqrt(mean) for mean in counts]\n",
    "\n",
    "time = zenithData[:,2] # seconds\n",
    "timeUncertainty = 1 # seconds, based on my ability to stop the timer and the couter at the same time.\n",
    "\n",
    "countRates = [c / t for c, t in zip(counts, time)]\n",
    "countRateUncertainties = [np.sqrt(timeUncertainty**2 * c**2 / t**4 + uc**2 / t**2) for c, uc, t in zip(counts, countsUncertainties, time)]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
    "\n",
    "ax.errorbar(angles, countRates, xerr=anglesUncertainty, yerr=countRateUncertainties, fmt=\".\", linewidth=1, capsize=3, color=\"cornflowerblue\")\n",
    "ax.set_xlabel(\"Zenith Angle ($^\\circ$)\")\n",
    "ax.set_ylabel(\"Countrate (counts / second)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "JUPiTer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
